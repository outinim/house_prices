{"cells":[{"metadata":{"_uuid":"d2623ff5-5c02-445f-84eb-a39b9fcfd30a","_cell_guid":"bbc48ac4-0585-4810-a162-71d304e10b11","trusted":true},"cell_type":"markdown","source":"In this notebook, we will present how to classify flower images by using transfer learning from a pre-trained network. \n\nA *pre-trained model* is a saved network model that was previously trained on a large dataset.\n\nThe idea of **transfer learning** for image classification is that if we use a model which was trained on a really large and representative dataset, this model can serve as a base model to classify images. Indeed, we can use the learned feature maps without having to start from scratch (which require to build and train a custom model on large datasets) which can take quiet a time (training time).\n\nHere, we're gonna test two approaches:\n1. **feature extraction**: we use the representations learned by an already trained network to extract meaningful features from new samples. We're simply gonna add a new classifier, which will be trained on top of the pretrained model so that we can use the feature maps learned previously for the dataset. Of course we do not have to retrain this trained network. The base convolutional network already contains features that are generically useful for classifying pictures. Note that the final classification part of the pretrained model is specific to the original classification.\n\n2. **fine tuning** : this method consists by unfreezing some of the top layers of the previously frozen model and jointly train both the new-top layer (to classify our specific datasets) and these last layers of the frozen model. We are doing this since only the last layers of the base model extract top-level feature maps, the first convolution layers only extract basic features (edges, vertical/horizontal lines ...). This fine-tuning of the top feature representation in the base model allows to make them more specific for our classification task."},{"metadata":{"_uuid":"032b908b-572a-4e53-9d0b-8ff77f47fde6","_cell_guid":"c90c3816-765a-4d64-aa05-92a2f24c2c9b","trusted":true},"cell_type":"markdown","source":"[ **I ) Introduction**](#content1)\n- VGG16\n- VGG19 \n- MobileNetV2\n\n[ **II ) Data**](#content2)\n- 2.1 Load & explore data\n- 2.2 Split training and validation set\n\n[ **III ) CNN model**](#content3)\n- 3.1 About the optimizer and learning rate\n- 3.2 Define the model\n- 3.3 Data augmentation\n- 3.4 Feature extraction\n- 3.5 Fine tuning\n\n[ **IV ) Model evaluation **](#content4)\n- Confusion matrix \n- Prediction vizualisations\n\n[ **V ) Conclusion **](#content5)"},{"metadata":{"_uuid":"e2bc1180-eeb8-4371-acb2-d2d76e9a1562","_cell_guid":"d008ae8c-c795-4ec5-8fc3-0f4f015b0bce","trusted":true},"cell_type":"code","source":"# import stuff \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nfrom PIL import Image\nfrom IPython.display import Image, display\nimport os\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow_hub as hub\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.applications import ResNet50, VGG16, VGG19, MobileNetV2\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as prepro_res50\nfrom tensorflow.keras.applications.vgg19 import preprocess_input as prepro_vgg19\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, GlobalAveragePooling2D, BatchNormalization, Dropout, MaxPool2D, MaxPooling2D","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6088ef73-fa05-4cbf-8fc6-e35053760528","_cell_guid":"68b6a569-9210-4b2b-b795-cc14ef08c274","trusted":true},"cell_type":"code","source":"# load the backend\nfrom keras import backend as K\n\n# prevent Tensorflow memory leakage\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2fa3c3d-2bd3-4952-866c-af3add9e16d3","_cell_guid":"9d436aa3-12b2-4282-9185-df7fbf7fd5ff","trusted":true},"cell_type":"code","source":"print(os.listdir('../input/'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b2f9e35-5f4c-45f9-9acd-2e276beefd28","_cell_guid":"1bbf5cda-6a97-4400-81b8-bdf4843c35f6","trusted":true},"cell_type":"markdown","source":"<a id=\"content1\"></a>\n## I) Introduction"},{"metadata":{"_uuid":"75f1c2ce-28cb-49f3-8aa3-4fe9780e8225","_cell_guid":"42fdefaf-fb3b-47f2-ae3e-3a14ac39f421","trusted":true},"cell_type":"markdown","source":"We show the architecture of one of the most commons CNN: VGG16. There are also VGG19, ResNet50, MobileNetV2, AlexNet etc. All these were pre-trained on the ImageNet dataset: a gold mine dataset for computer vision. It consists of about 14 M hand-labelled annotated images which contains over 22,000 categories. This pre-trained models will be a solid base to help us classify our flower dataset.\n\nVGG16 was published in 2014 and is one of the simplest (compared to the other CNN architectures used in Imagenet competition). This network contains total 16 layers in which weights and bias parameters are learnt.\n- a total of 13 convolutional layers are stacked and 3 dense layers for classification\n- a number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder)\n- the informative features are obtained by Max Pooling layers applied at different steps in the architecture\n- the dense layers are made of 4096, 4096, and 1000 nodes"},{"metadata":{"_uuid":"c1bc1200-0a1f-4608-986e-90c4d076f316","_cell_guid":"9eebba3a-6525-47a1-8601-361126463c7d","trusted":true},"cell_type":"markdown","source":"![alt text](https://tech.showmax.com/2017/10/convnet-architectures/image_0-8fa3b810.png 'VGG16 architecture')"},{"metadata":{},"cell_type":"markdown","source":"So if you want to implement yourself a VGG16-like model it is quite straighforward. The only issue will be the time to train the model. Nonetheless, we demonstrates how to build it below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's implement it  \ninput_shape = (224, 224, 3)\n\nmy_VGG16 = Sequential([Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'), \n                       Conv2D(64, (3, 3), activation='relu', padding='same'), \n                       MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n                       Conv2D(128, (3, 3), activation='relu', padding='same'), \n                       Conv2D(128, (3, 3), activation='relu', padding='same'), \n                       MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n                       Conv2D(256, (3, 3), activation='relu', padding='same'),  \n                       Conv2D(256, (3, 3), activation='relu', padding='same'), \n                       Conv2D(256, (3, 3), activation='relu', padding='same'),  \n                       MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n                       Conv2D(512, (3, 3), activation='relu', padding='same'), \n                       Conv2D(512, (3, 3), activation='relu', padding='same'),  \n                       Conv2D(512, (3, 3), activation='relu', padding='same'),  \n                       MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n                       Conv2D(512, (3, 3), activation='relu', padding='same'), \n                       Conv2D(512, (3, 3), activation='relu', padding='same'), \n                       Conv2D(512, (3, 3), activation='relu', padding='same'), \n                       MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n                       Flatten(),                          # Convert 3D matrices into 1D vector\n                       Dense(4096, activation='relu'),     # Add Fully-connected layers\n                       Dense(4096, activation='relu'), \n                       Dense(1000, activation='softmax')   # Final Fully-connected layer for predictions\n                      ])\n\nmy_VGG16.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13b95255-5c5b-4fc9-9fac-8d60e9186b8a","_cell_guid":"35e3d8f2-a7d5-4862-a20e-d02e63b30f6e","trusted":true},"cell_type":"markdown","source":"We also present the VGG19 model. It is similar to the VGG16 architecture with the addition of 3 more convolution layers."},{"metadata":{"_uuid":"66ef5c1f-f0f7-4259-98e3-ab95de256e09","_cell_guid":"d3c1d15f-e544-4145-8510-ce8ab4132d25","trusted":true},"cell_type":"markdown","source":"![alt text](https://cdn-images-1.medium.com/max/1600/1*cufAO77aeSWdShs3ba5ndg.jpeg 'VGG19 architecture')"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we'll make use of a much complex architecture with the MobileNetV2 model. It has been developed in 2018, MobileNetV2 is a significant improvement over MobileNetV1 and pushes the state of the art for mobile visual recognition including classification, object detection and  segmentation. If you're interested, you can find more details in the [release paper](https://arxiv.org/abs/1801.04381)."},{"metadata":{"_uuid":"66ef5c1f-f0f7-4259-98e3-ab95de256e09","_cell_guid":"d3c1d15f-e544-4145-8510-ce8ab4132d25","trusted":true},"cell_type":"markdown","source":"![alt text](https://1.bp.blogspot.com/-M8UvZJWNW4E/WsKk-tbzp8I/AAAAAAAAChw/OqxBVPbDygMIQWGug4ZnHNDvuyK5FBMcQCLcBGAs/s1600/image5.png 'MobileNetV2 architecture')"},{"metadata":{"_uuid":"3aeab0ef-4370-4c49-8ea6-1adaeb5d5332","_cell_guid":"ef8498aa-b031-4719-ab63-5b87fa40f113","trusted":true},"cell_type":"markdown","source":"<a id=\"content2\"></a>\n## II) Data"},{"metadata":{"_uuid":"1cfe85c3-13c6-4ef7-bdfd-0bb9ec55ec92","_cell_guid":"fdec00d5-e638-45c7-96d3-36c3ee28ea63","trusted":true},"cell_type":"markdown","source":"### 2.1 - Load & explore data"},{"metadata":{"_uuid":"4ada82fc-1f13-412b-8b6d-d906cfb0bb09","_cell_guid":"71e440b8-ce8c-4a19-863c-85c94fe1a2bd","trusted":true},"cell_type":"code","source":"path_data = '../input/flowers-recognition/flowers/flowers/'\nprint(os.listdir(path_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f52cca2-a1e5-431e-9f6d-05ece74bb81a","_cell_guid":"ae82948b-fa5f-40a0-8f51-465f58a8b942","trusted":true},"cell_type":"code","source":"from os.path import join\n\nimg_folders = [join(path_data, folder) for folder in os.listdir(path_data)]\nlist(img_folders)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7038df86-c3b6-4f7e-bef9-f5644eebdc01","_cell_guid":"c8d56e62-c7a1-47d0-8776-17d24a69ffe6","trusted":true},"cell_type":"code","source":"data_dir = '../input/flowers-recognition/flowers/flowers/'\n\ndata = load_files(data_dir, random_state=28, shuffle=True)\nX = np.array(data['filenames'])    # files location of each flower\ny = np.array(data['target'])       # target label of each flower\nlabels = np.array(data['target_names'])\n\n# remove eventual .pyc or .py files\npyc_file = (np.where(file==X) for file in X if file.endswith(('.pyc','.py')))\nfor pos in pyc_file:\n    X = np.delete(X, pos)\n    y = np.delete(y, pos)\n    \nprint(f'Data files - {X}')\nprint(f'Target labels - {y}')   # numbers are corresponding to class label, \n                               # we have to change them to a vector of 5 elements\nprint(f'Name labels - {labels}')\nprint(f'Number of training files : {X.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa142182-7d5d-4290-a158-7878b679fdbe","_cell_guid":"4a356019-c573-4c18-b80e-ec98ecd08f37","trusted":true},"cell_type":"code","source":"# Flower species number\ndf = pd.DataFrame({'species': y})\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9c10b4-331e-44ca-b436-67916e01b596","_cell_guid":"d04bbcbd-6451-4ff4-ae43-91917e24d933","trusted":true},"cell_type":"code","source":"# associate names to species number\ndf['flower'] = df['species'].astype('category')\ndf['flower'].cat.categories = labels\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58a79fba-3a28-431f-8740-84397bb49185","_cell_guid":"12eef245-10d8-417a-979b-8c2044df5b6e","trusted":true},"cell_type":"markdown","source":"Let's check how many of each species of flowers are present."},{"metadata":{"_uuid":"f2d65762-b6f8-4a71-930a-3689507b94f5","_cell_guid":"2b376344-24a1-4adf-b9c4-14a04f011be4","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax = sns.countplot(x=\"flower\", data=df)\nax.set(ylabel='Count', title='Flower species distribution')\nax.tick_params(axis='x', rotation=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af407f5d-99d3-498d-8cee-53b428617c2d","_cell_guid":"a0f113c8-f54a-4994-b4a3-90243fd786af","trusted":true},"cell_type":"markdown","source":"Now, we're gonna load the different images and transform them into numpy arrays."},{"metadata":{"_uuid":"a2431bc9-8e21-4eb7-9c92-1c388627efc4","_cell_guid":"85009bc5-887a-41f2-8bfb-d81c2f9b26b2","trusted":true},"cell_type":"code","source":"image_size = 224     # standard value for Transfer learning usecase (MobileNet, ResNet50, VGG16, VGG19)\n\ndef read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]   # load image\n    img_array = np.array([img_to_array(img) for img in imgs])   # image to array \n    return(img_array)\n\nX = np.array(read_and_prep_images(X))\nprint(X.shape)  # (4323, 224, 224, 3) = (num_images, height_size, width_size, depth=RGB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dc22c73-3698-4a84-b675-14dfb84cf6ad","_cell_guid":"f62f4de6-3853-452d-9207-a286100f0add","trusted":true},"cell_type":"code","source":"# Let's have a look at 6 randomly picked flowers.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ddc2ea6-63cf-4d47-8d93-36871626a7e6","_cell_guid":"9898bcb9-766d-43d0-9b09-5ab2ceac5bdc","trusted":true},"cell_type":"code","source":"N = 18  # flowers to display\nfig, axes = plt.subplots(3, 6, figsize=(16,6))\nfor ax, j in zip(axes.flat, np.random.randint(0, len(X), N)):    \n    ax.imshow(X[j].astype(np.uint8))\n    ax.set(title=f'Flower: {labels[y[j]]}', xticks=[], yticks=[])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54c512f4-7460-453a-9cd1-d6b6d7169c02","_cell_guid":"67758a31-b6d0-4423-9ddb-35b336a72cc2","trusted":true},"cell_type":"markdown","source":"### 2.2 - Label encoding"},{"metadata":{"_uuid":"34b8d053-6135-4e91-8315-a746d225a3cb","_cell_guid":"74faf318-831d-4349-834e-68c8881ca863","trusted":true},"cell_type":"code","source":"num_classes = len(np.unique(y))\nprint(f'Number of classes: {num_classes} --> {labels}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65f239f2-5d94-4ee4-85d0-d2d06e9a7f47","_cell_guid":"daea4e9b-a5c1-4929-b1c8-4fb0cead4776","trusted":true},"cell_type":"markdown","source":"Labels are the 5 species number (from 0 to 4). Thus, we need to encode these labels to one-hot vectors. For instance, an image of a sunflower should have a label 3 and a corresponding **y** = [0,0,0,1,0]."},{"metadata":{"_uuid":"30dcf4f3-abdb-4ab0-8f2e-5a19f383438b","_cell_guid":"6b2b5201-8da1-4704-a974-8864989394a8","trusted":true},"cell_type":"code","source":"y = to_categorical(y, num_classes)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"855fc5f3-9f74-46b3-8919-b70ff579c0a9","_cell_guid":"2c2c6b04-556a-44b6-a4a4-472a51785e70","trusted":true},"cell_type":"markdown","source":"### 2.3 - Split training and validation set"},{"metadata":{"_uuid":"d4257764-428d-4a22-959c-56b7e8b25916","_cell_guid":"3384f542-fd41-4594-976c-5ac2482a3555","trusted":true},"cell_type":"markdown","source":"Here, we're gonna split our dataset into a training, a validation and a testing one. This ensures that there are no bias: the model is trained on images with known labels, then we test our model accuracy on the validation dataset on images that our model did not see before. Finally, we compute the accuracy on the test dataset."},{"metadata":{"_uuid":"8dc55c67-4405-4707-acea-62a189e11eec","_cell_guid":"2c995811-acbe-41e8-bd41-b98e33aad26d","trusted":true},"cell_type":"code","source":"#train, validation and test from the train dataset\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, shuffle=True, \n                                                test_size=0.25, random_state=28)\n\nXval, Xtest, yval, ytest = train_test_split(Xtest, ytest, test_size=0.5,\n                                            shuffle=True, random_state=28)\nprint(f'Train dataset: {Xtrain.shape[0]}')\nprint(f'Validation dataset: {Xval.shape[0]}')\nprint(f'Test dataset: {Xtest.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d61ddfab-8ff4-48d5-ba19-6815c64bfbe0","_cell_guid":"e4077cd0-0e6a-4bbe-9fad-9cd8b0703d5e","trusted":true},"cell_type":"code","source":"# release memory\ndel X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c4baf76-afed-4862-9799-d455e63c6046","_cell_guid":"dbee8d4c-e94c-4b51-ae44-eff007bf2daf","trusted":true},"cell_type":"code","source":"# rescale pixel values\n# X /= 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea41d826-beef-4607-a826-795e7665da57","_cell_guid":"23cd7e27-2b06-40d9-aa6a-e8f73b5c04ae","trusted":true},"cell_type":"code","source":"#num_classes = 5\n#resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"301c4ddf-44ad-40e9-b66a-732d0d2c1a3f","_cell_guid":"f85ef329-1bbb-491a-bba9-5e0092708351","trusted":true},"cell_type":"markdown","source":"<a id=\"content3\"></a>\n## III) CNN model"},{"metadata":{"_uuid":"025d2508-6605-4574-89bd-94dbb63c1000","_cell_guid":"3828a162-ec80-43d8-8953-bc8bb420fac2","trusted":true},"cell_type":"markdown","source":"### 3.1 About the optimizer and learning rate"},{"metadata":{"_uuid":"addd81a0-bd6c-41d9-979a-81ff641deef2","_cell_guid":"5a789e64-7523-48b3-ac7c-fd3ab92db82c","trusted":true},"cell_type":"markdown","source":"When our model will be built, we need to specify an accuracy function, a loss function and an optimisation algorithm.\n\nThe accuracy function is used to evaluate the performance of the model.\n\nThe loss function is used to measure how the model performs on data with known labels. It tells us how poorly the model performs in a supersised system. For multi-label classification, we make use of a specific loss function called as *categorical_crossentropy* (similar to cross-entropy in maths).\n\nFinally, the optimizer function is used in order to minize the loss function by changing model parameters (weighs values, filters kernel values etc.). \n\nFor this classification problem, we choose the `RMSprop` optimizer which is very efficient and commonly used (more details on the [optimizers on Keras here](https://keras.io/optimizers/))."},{"metadata":{"_uuid":"4f1c8ab8-d106-4116-b679-0e0c79f2f651","_cell_guid":"dd504d16-4706-474c-8d83-fb4647f7f855","trusted":true},"cell_type":"markdown","source":"Since deep networks can take quiet a time for the optimizer to converge, we're gonna use an annealing method of the learning rate (*LR*).\n\nThe *LR* is basically the step by which the optimizer is 'walking'. A hight *LR* correspond to big steps and thus the convergence is faster. However, in that case the sampling is not really efficient since the optimizer do not fall especially in the right minima.\n\nAt the opposite, have a low *LR* means that the optimizer will probably find the right local minima but it will take a lot of time. \n\nThe idea here is to start from a low value but not so low and then decrease the *LR* along the training to reach efficiently the global minimum of the loss function. Using the `ReduceLROnPlateau` method , we are able to choose to reduce the *LR* by a coefficient (here 75%) if the accuracy has not improved after a number of epochs (here 3).\n\n<br>\n\nIn addition, we use the `EarlyStopping` method to control the training time: if the accuracy has not improved after 5 epochs we stop.\n\nFinally we make use of the `ModelCheckpoint` which is useful for monitoring the best found weights during the training."},{"metadata":{"_uuid":"8ce11503-72c7-4e7e-a2cc-0532bbaa53c1","_cell_guid":"16612f1a-1c8c-418a-bcf6-edbd73129afd","trusted":true},"cell_type":"markdown","source":"### 3.2 Define the model"},{"metadata":{"_uuid":"135a74ea-849b-4530-8ea8-03c2a5d02671","_cell_guid":"7ede296f-586a-488e-89e2-ec4dacd4c915","trusted":true},"cell_type":"markdown","source":"For now, we're doing feature extraction i.e. we freeze the convolutional base (MobileNet). Then, we add a classifier on top of it and train this top-level classifier."},{"metadata":{"_uuid":"7f0f49cb-75cf-442e-b531-a4bfe6a49539","_cell_guid":"45b95be5-7145-426a-8bac-50374c5072b1","trusted":true},"cell_type":"code","source":"# Load the VGG19 model without the final layers (include_top=False)\nimg_shape = (image_size, image_size, 3)\n\nprint('Loading MobileNetV2 ...')\nbase_model = MobileNetV2(input_shape=img_shape,\n                   include_top=False,\n                   weights='imagenet')\nprint('MobileNetV2 loaded')\n\nbase_model.trainable = False\n    \n#base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d147986-74a7-4cd0-aa49-1e08e1ce7256","_cell_guid":"ea945e30-d316-418e-80bb-78d004b29777","trusted":true},"cell_type":"code","source":"base_model.output_shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8fab957-abd9-45ae-be8e-d018caaeffe0","_cell_guid":"1f780748-7fef-4c99-94d8-1b35556e5feb","trusted":true},"cell_type":"markdown","source":"Now, we need to generate predictions from the block of features, average over the spatial locations, using a `GlobalAveragePooling2D` layer to convert the features to a single 1280-element vector per image. Finally, we'll some regular `Dense` layer with a final one with 5 units corresponding to each species of flower."},{"metadata":{"_uuid":"89d3150d-c0a2-4145-92ff-72117517ffdb","_cell_guid":"534341d5-7149-4a5a-ac3d-c50a9f8652fc","trusted":true},"cell_type":"code","source":"model = Sequential([base_model,\n                    GlobalAveragePooling2D(), \n                    Dense(num_classes, activation='softmax')\n                   ])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb482ef2-1d71-4daa-80db-f3704eae9de1","_cell_guid":"01f40f32-5f4e-4175-8743-6e0a15e3cfeb","trusted":true},"cell_type":"markdown","source":"Note that only ~ 6000 parameters will be trained, the other ~2.2M from the MobileNetV2 model were already trained."},{"metadata":{"_uuid":"dc30d2b2-806b-4257-8a4f-04adbcc2f715","_cell_guid":"fc6dae32-688c-4a1b-b9f2-36c760396e77","trusted":true},"cell_type":"code","source":"# callbacks \nweight_path = '{}_best_weights.hdf5'.format('flower')\ncheckpointer = ModelCheckpoint(weight_path,\n                               monitor='val_accuracy',\n                               verbose=1, \n                               save_best_only=True,\n                               mode='auto',\n                               save_weights_only=True)\n\n# set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.7, \n                                            min_lr=0.00001)\n    \n# early stop if not improvement of accuracy after 5 epochs\nearly = EarlyStopping(patience=6, \n                      verbose=1) \n    \ncallbacks = [checkpointer, learning_rate_reduction] #, early]\n\n# Optimizer\nopt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    \n# Compilation\nmodel.compile(optimizer=opt, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07adb659-828c-47d7-b191-15a8a2d7c7ef","_cell_guid":"dc76cf5e-7463-4176-a9ce-5817686d8396","trusted":true},"cell_type":"markdown","source":"### 3.3 - Data augmentation"},{"metadata":{"_uuid":"5a2e8b1b-9e8f-453e-b9cf-fd5051765dd9","_cell_guid":"2668787f-1bc9-4e8f-b698-0a50295dd844","trusted":true},"cell_type":"markdown","source":"A useful trick to ovoid any overfitting is to use *data augmentation*. What is that? Well, the idea is to add artificially data into our dataset. But of course not any data, we alter the dataset with tiny transformations to reproduce very similar images. \n\nFor instance, we rotate of a few degree an image, we de-center it or we zoom in or out a little bit. These common augmentation techniques are horizontal/vertical flips, rotations, translations, rescaling, random crops, adjust brightness and more.\n\nThanks to these transformations, we can get bigger dataset (x2, x3 in size) and then train our model in a much robust way."},{"metadata":{"_uuid":"fcef0534-3d3f-4f79-a3b7-43e70f529af2","_cell_guid":"44a9e69b-089f-40ba-990f-19c82ad683bf","trusted":true},"cell_type":"code","source":"image_size = 224\nbatch_size = 32\npath = '../input/flowers-recognition/flowers/flowers/'\n\n#train_gen = train_aug.flow(Xtrain, ytrain, batch_size=batch_size)\n# The validation data must not have data augmentation\n#valid_gen = valid_no_aug.flow(Xval, yval, batch_size=batch_size)\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,           # rescale pixel values [0,255] to [0,1]\n        horizontal_flip=True,     # random horizontal flip\n        width_shift_range=0.2,    # random shift images horizontally (fraction of total width)\n        height_shift_range=0.2,   # random shift images vertically (fraction of total height)\n        zoom_range=0.2)           # random zoom image\n        #rotation_range=20,       # random rotation\n        #shear_range=0.2)         # shear transfo\n        #validation_split=0.2)    # splitting train / test datasets\n\ntest_datagen = ImageDataGenerator(\n        rescale=1./255)\n        #validation_split=0.2)\n\ntrain_gen = train_datagen.flow(\n        Xtrain, ytrain, \n        batch_size=batch_size,\n        shuffle=False)              # already applied\n\nvalid_gen = test_datagen.flow(\n        Xval, yval,\n        batch_size=batch_size,\n        shuffle=False)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a5dac1-e10f-473a-a679-7f4fe72dcb87","_cell_guid":"2656b23a-1d7e-4af5-b86c-1e48e310c447","trusted":true},"cell_type":"markdown","source":"### 3.4 Feature extraction"},{"metadata":{"_uuid":"9737f711-b469-4eb7-bef2-24de47a8f972","_cell_guid":"2056d6e6-bb86-41d0-8ef3-2e24a51039ae","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"batch_size = 32\nepochs_0 = 80\nsteps_per_epoch = len(train_gen.x) // train_gen.batch_size\nvalidation_steps = len(valid_gen.x) // valid_gen.batch_size\n\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=len(Xtrain) // batch_size,   # or batch_size=32\n    epochs=epochs_0 ,\n    validation_data=valid_gen,\n    validation_steps=len(Xval) // batch_size,\n    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3b71819-c9ea-4890-bec2-f362406340d4","_cell_guid":"3bd59d62-8195-4b3f-a358-c7f93228fdd9","trusted":true},"cell_type":"code","source":"def plot_history(history, loss_max=5):\n    \"\"\"\n    Check loss and accuracy evolution.\n    \"\"\"\n    \n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14, 4))\n    ax1.plot(acc, label='Training')\n    ax1.plot(val_acc, label='Validation')\n    ax1.legend(loc='lower right')\n    ax1.set(ylabel='Accuracy', title='Training - Validation Accuracy', \n            ylim=([min(plt.ylim()),1]))\n\n    ax2.plot(loss, label='Training')\n    ax2.plot(val_loss, label='Validation')\n    ax2.legend(loc='upper right')\n    ax2.set(ylabel='Loss (cross entropy)', xlabel='epochs',\n           title='Training - Validation Loss', ylim=([0, loss_max]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5de33947-816e-4e03-a589-3b84683a2360","_cell_guid":"f6ffcc30-211e-4f18-b1ef-4dd2465a8a1b","trusted":true},"cell_type":"code","source":"plot_history(history, loss_max=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generator for test dataset\ndatagen = ImageDataGenerator(\n        rescale=1./255)\n\neval_datagen = datagen.flow(\n        Xtest, ytest,\n        batch_size=batch_size,\n        shuffle=False)      # since shuffle was already during splitting into train, valid, test\n\n# Evaluation on the test dataset\nloss, acc = model.evaluate_generator(eval_datagen, verbose=0)\nprint(f'Test loss: {loss:.2f}')\nprint(f'Test accuracy: {acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e46b7087-6af1-4009-a862-d26408dba597","_cell_guid":"16dd8a2f-9842-41a5-bfd0-2a238cfe937e","trusted":true},"cell_type":"markdown","source":"### 3.5 Fine tuning"},{"metadata":{},"cell_type":"markdown","source":"It is now time for the fine tuning of our model: we're gonna unfreeze some of the top layers of the base model and train all those and the top layer classifier."},{"metadata":{"_uuid":"47ac14a2-87be-4a07-99da-60267adffd7a","_cell_guid":"a05c7295-749d-40c4-b3bc-1b6ad3f9da49","trusted":true},"cell_type":"code","source":"base_model.trainable = True\n\n# Let's take a look to see how many layers are in the base model\nprint(f'Number of layers in the base model: {len(base_model.layers)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bac3b7b8-5541-4ba4-a631-edd915c651e2","_cell_guid":"6c24e385-0114-45f7-8981-8120b244f92d","trusted":true},"cell_type":"code","source":"# Fine-tune from this layer onwards\nfine_tuning = 100\n\n# Freeze all the layers before fine_tuned_ind\nfor layer in base_model.layers[:fine_tuning]:\n    layer.trainable =  False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7dfeb0-0a31-4e8d-94b0-d6aa977c478b","_cell_guid":"9e0402a8-fbe8-4e50-a952-71e85aedfc28","trusted":true},"cell_type":"code","source":"# Load best weights\n# model.load_weights(weight_path)\n\n# Finer learning rate now\nopt = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n    \n# Compilation\nmodel.compile(optimizer=opt, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3292121-29b6-4a72-955d-1e0c654e6b11","_cell_guid":"cefaafcd-e5c8-4f61-9e92-17e8c391edc1","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d6aaf02-9fe8-4f10-b8ee-5520e1de7a9f","_cell_guid":"0e76adc4-f8c1-4c75-9a5f-e88a1878c259","trusted":true},"cell_type":"markdown","source":"Continue training the model"},{"metadata":{"_uuid":"0f0804f7-e670-430c-b5e3-7e4b38764365","_cell_guid":"2b2c02bd-d57c-47a3-90af-5453057540e6","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"fine_tuned_epochs = 60\ntotal_epochs = epochs_0 + fine_tuned_epochs\n\nhistory_fined = model.fit_generator(\n    train_gen,\n    steps_per_epoch=len(Xtrain) // batch_size,   # or batch_size=32\n    epochs=total_epochs,\n    initial_epoch=history.epoch[-1],\n    validation_data=valid_gen,\n    validation_steps=len(Xval) // batch_size,\n    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01e42a3b-707f-4159-965d-10d162d9f919","_cell_guid":"3c993632-44de-4c58-b151-49e2e4d53fec","trusted":true},"cell_type":"code","source":"def plot_history_fined(history, history_fined, initial_epochs=epochs_0, loss_max=1):\n    \"\"\"\n    Check loss and accuracy evolution after fine tuning\n    \"\"\"\n    \n    acc = history.history['accuracy'][:epochs_0]\n    acc += history_fined.history['accuracy']\n    val_acc = history.history['val_accuracy'][:epochs_0]\n    val_acc += history_fined.history['val_accuracy']\n    \n    loss = history.history['loss'][:epochs_0]\n    loss += history_fined.history['loss']\n    val_loss = history.history['val_loss'][:epochs_0]\n    val_loss += history_fined.history['val_loss']\n  \n    \n    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14, 4))\n    ax1.plot(acc, label='Training')\n    ax1.plot(val_acc, label='Validation')\n    ax1.plot([initial_epochs-1,initial_epochs-1],\n              plt.ylim(), label='fine-tuning', ls='--')\n    ax1.legend(loc='lower right')\n    ax1.set(ylabel='Accuracy', title='Training - Validation Accuracy', \n            ylim=([0.4,1.01]))\n\n    ax2.plot(loss, label='Training')\n    ax2.plot(val_loss, label='Validation')\n    ax2.plot([initial_epochs-1,initial_epochs-1],\n              plt.ylim(), label='fine-tuning', ls='--')\n    ax2.legend(loc='upper right')\n    ax2.set(ylabel='Loss (cross entropy)', xlabel='epochs',\n           title='Training - Validation Loss', ylim=([0, loss_max]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeb20f52-61fe-40e4-ab59-badaba74c05e","_cell_guid":"b7426d25-c3a9-4969-b217-481f1911e04f","trusted":true},"cell_type":"code","source":"plot_history_fined(history, history_fined)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great ! We can really see that fine-tuning is working and improve the accuracy of our model. We note also that the validation loss tends to increase a bit a the end: to prevent an eventual overfitting situation, we could add the `EarlyStopping` function in the callbacks during the training."},{"metadata":{"_uuid":"0dd6cfb7-5493-44bb-addd-9a312c47b293","_cell_guid":"41ea7764-c4fd-4f21-9bcc-d68d80369d1a","trusted":true},"cell_type":"markdown","source":"<a id=\"content4\"></a>\n## IV) Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation on the test dataset\nloss, acc = model.evaluate_generator(eval_datagen, verbose=0)\nprint(f'Test loss: {loss:.2f}')\nprint(f'Test accuracy: {acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed we have now an 88% accuracy on the test dataset (compared to 77% before fine tuning) ! "},{"metadata":{"_uuid":"82e1429c-f55c-4d59-b44e-48b68f30a7ba","_cell_guid":"b6ec9c05-88f5-4b16-a556-c3b94579a9e6","trusted":true},"cell_type":"markdown","source":"### Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn import metrics\n\npred = model.predict(eval_datagen, verbose=1)\n\n# get most likely class\ny_pred = pred.argmax(axis=1)\ny_true = ytest.argmax(axis=1)\n\nprint(metrics.classification_report(y_true, y_pred))\n\n# confusion matrix\nmat = metrics.confusion_matrix(y_true, y_pred)\ndf_mat = pd.DataFrame(mat, index=labels, columns=labels)\nplt.figure(figsize=(8,6))\nsns.heatmap(df_mat, annot=True, fmt='d', cmap=plt.cm.Reds)\n#plt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###Â Prediction vizualisations"},{"metadata":{},"cell_type":"markdown","source":"let's have a look at some predictions !"},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 20  # flowers to display\nfig, axes = plt.subplots(4, 5, figsize=(20,12))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(Xtest[i].astype(np.uint8))\n    ax.set(xticks=[], yticks=[])\n    true = y_true[i]\n    prediction = y_pred[i]   \n    ax.set_xlabel(f'Predict: {labels[prediction]}\\n True: {labels[true]}', \n                  color='black' if true == prediction else 'red')\n\n#fig.tight_layout()\nfig.suptitle('Predicted flowers; Incorrect Labels in Red', size=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb8545ca-2cfa-472f-8ddf-c2227d897029","_cell_guid":"05abac08-963a-4849-a8bb-83b43b9180b7","trusted":true},"cell_type":"markdown","source":"<a id=\"content5\"></a>\n## Conclusion\n\nWe can note the improvement of the model predictions by doing some fine-tuning. Of course, we can complexify the model by playing with the hyperparameters and/or adding other layers on top of the top-less MobileNetV2 base model such as several `Dense` layers with some `Dropout` ones. Feel free to test some different architectures. \n\nIt will be interesting also to compare this fine-tuning method with VGG16, VGG19, ResNet50 etc. models.\n\nLet me know what you thought of this notebook and if it pleased you don't hesitate to leave me a comment with a +1 ; ) "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}